# -*- coding: utf-8 -*-
"""video action recognition using MMAction2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cB92M64nHaRx8WHCG2NGnAGa8ySv5AGZ

#  Using MMAction2 to solve video action recognition problem 

Train a  recognizer with a new dataset(small Dataset)

## Install MMAction2
"""

# Check nvcc version
!nvcc -V
# Check GCC version
!gcc --version

# Commented out IPython magic to ensure Python compatibility.
# install dependencies: (use cu111 because colab has CUDA 11.1)
!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html

# install mmcv-full thus we could use CUDA operators
!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html

# Install mmaction2
!rm -rf mmaction2
!git clone https://github.com/open-mmlab/mmaction2.git
# %cd mmaction2

!pip install -e .

# Install some optional requirements
!pip install -r requirements/optional.txt

# Check Pytorch installation
import torch, torchvision
print(torch.__version__, torch.cuda.is_available())

# Check MMAction2 installation
import mmaction
print(mmaction.__version__)

# Check MMCV installation
from mmcv.ops import get_compiling_cuda_version, get_compiler_version
print(get_compiling_cuda_version())
print(get_compiler_version())

"""## Train a recognizer on customized dataset

To train a new recognizer, there are usually three things to do:
1. Support a new dataset
2. Modify the config
3. Train a new recognizer

### Support a new dataset

Firstly, let's download a small dataset obtained from following github link . We select 80 videos with their labels as train dataset and 20 videos with their labels as test dataset.
"""

##cloning the data from Github
!git clone https://github.com/IvoryCandy/Skyrim-Human-Actions.git

# Check the directory structure of the data
# Install tree first
!apt-get -q install tree
!tree Skyrim-Human-Actions

# directories to store training and validation data 
! mkdir /content/mmaction2/Skyrim-Human-Actions_final
! mkdir /content/mmaction2/Skyrim-Human-Actions_final/train
! mkdir /content/mmaction2/Skyrim-Human-Actions_final/val

import os
import shutil

# filing  the anotation flies and the train and validation folders 
dir="/content/mmaction2/Skyrim-Human-Actions/"
l=0
for folder in os.listdir(dir):
  t=0
  if folder==".git" or folder=='.gitignore' or folder=='extract_frame.py'or folder=='README.md'or folder=='LICENSE':
    continue 
  for i in os.listdir(dir +'/'+folder):
    if t<8:
      # Open the file in append & read mode ('a+')
      with open("/content/mmaction2/Skyrim-Human-Actions_final/train.txt", "a+") as file_object:
        file_object.seek(0)
        data = file_object.read(100)
        if len(data) > 0 :
          file_object.write("\n")
      # Append text at the end of file
        append=str(i)+" "+str(l)
        file_object.write(str(i)+" "+str(l))
        file_object.close()
      src = os.path.join(dir + '/'+folder,i)
      #dst = os.path.join("/content/Skyrim-Human-Actions_final/train", i)
      dst="/content/mmaction2/Skyrim-Human-Actions_final/train/"
      shutil.copy(src, dst)
      t+=1
    else:
      with open("/content/mmaction2/Skyrim-Human-Actions_final/val.txt", "a+") as file_object:
        file_object.seek(0)
        data = file_object.read(100)
        if len(data) > 0 :
          file_object.write("\n")
      # Append text at the end of file
        append=str(i)+" "+str(l)
        file_object.write(append)
        file_object.close()
      src = os.path.join(dir + '/'+folder,i)
      #dst = os.path.join("/content/Skyrim-Human-Actions_final/test", i)
      dst="/content/mmaction2/Skyrim-Human-Actions_final/val/"
      shutil.copy(src, dst)
  l+=1

"""### Modify the config

In the next step, we need to modify the config for the training.
To accelerate the process, we finetune a recognizer using a pre-trained recognizer.
"""

!mkdir checkpoints
!wget -c https://download.openmmlab.com/mmaction/recognition/tsn/tsn_r50_1x1x3_100e_kinetics400_rgb/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth \
      -O checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth

"""Given a config that trains a TSN model on kinetics400-full dataset, we need to modify some values to use it for training TSN on Kinetics400-tiny dataset.

"""

from mmcv.runner import set_random_seed
# Modify dataset type and path
from mmcv import Config
cfg = Config.fromfile('./configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py')
cfg.dataset_type = 'VideoDataset'
cfg.data_root = 'Skyrim-Human-Actions_final/train/'
cfg.data_root_val = 'Skyrim-Human-Actions_final/val/'
cfg.ann_file_train = 'Skyrim-Human-Actions_final/train.txt'
cfg.ann_file_val = 'Skyrim-Human-Actions_final/val.txt'
cfg.ann_file_test = 'Skyrim-Human-Actions_final/val.txt'

cfg.data.test.type = 'VideoDataset'
cfg.data.test.ann_file = 'Skyrim-Human-Actions_final/val.txt'
cfg.data.test.data_prefix = 'Skyrim-Human-Actions_final/val/'

cfg.data.train.type = 'VideoDataset'
cfg.data.train.ann_file = 'Skyrim-Human-Actions_final/train.txt'
cfg.data.train.data_prefix = 'Skyrim-Human-Actions_final/train/'

cfg.data.val.type = 'VideoDataset'
cfg.data.val.ann_file = 'Skyrim-Human-Actions_final/val.txt'
cfg.data.val.data_prefix = 'Skyrim-Human-Actions_final/val/'
#cfg.data.video_per_gpu=32

# The flag is used to determine whether it is omnisource training
cfg.setdefault('omnisource', False)
# Modify num classes of the model in cls_head
cfg.model.cls_head.num_classes = 10
# We can use the pre-trained TSN model
cfg.load_from = './checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'

# Set up working dir to save files and logs.
cfg.work_dir = './tutorial_exps'

# The original learning rate (LR) is set for 8-GPU training.
# We divide it by 8 since we only use one GPU.
cfg.data.videos_per_gpu = cfg.data.videos_per_gpu // 16
cfg.optimizer.lr = cfg.optimizer.lr / 8 / 16
cfg.total_epochs = 40
# We can set the checkpoint saving interval to reduce the storage cost
cfg.checkpoint_config.interval = 5
# We can set the log print interval to reduce the the times of printing log
cfg.log_config.interval = 5

# Set seed thus the results are more reproducible
cfg.seed = 0
set_random_seed(0, deterministic=False)
cfg.gpu_ids = range(1)

# Save the best
cfg.evaluation.save_best='auto'


# We can initialize the logger for training and have a look
# at the final config used for training
print(f'Config:\n{cfg.pretty_text}')

"""### Train a new recognizer

Finally, lets initialize the dataset and recognizer, then train a new recognizer!
"""

import os.path as osp

from mmaction.datasets import build_dataset
from mmaction.models import build_model
from mmaction.apis import train_model

import mmcv

# Build the dataset
datasets = [build_dataset(cfg.data.train)]

# Build the recognizer
model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))

# Create work_dir
mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))
train_model(model, datasets, cfg, distributed=False, validate=True)

"""### Understand the log
From the log, we can have a basic understanding the training process and know how well the recognizer is trained.

Firstly, the ResNet-50 backbone pre-trained on ImageNet is loaded, this is a common practice since training from scratch is more cost. The log shows that all the weights of the ResNet-50 backbone are loaded except the `fc.bias` and `fc.weight`.

Second, since the dataset we are using is small, we loaded a TSN model and finetune it for action recognition.
The original TSN is trained on original Kinetics-400 dataset which contains 400 classes but our dataset  have 10 classes. Therefore, the last FC layer of the pre-trained TSN for classification has different weight shape and is not used.

Third, after training, the recognizer is evaluated by the default evaluation. The results show that the recognizer achieves % top1 accuracy and % top5 accuracy on the val dataset,
 
Not bad!

## Test the trained recognizer

After finetuning the recognizer, let's check the prediction results!
"""

from mmaction.apis import single_gpu_test
from mmaction.datasets import build_dataloader
from mmcv.parallel import MMDataParallel

# Build a test dataloader
dataset = build_dataset(cfg.data.test, dict(test_mode=True))
data_loader = build_dataloader(
        dataset,
        videos_per_gpu=1,
        workers_per_gpu=cfg.data.workers_per_gpu,
        dist=False,
        shuffle=False)
model = MMDataParallel(model, device_ids=[0])
outputs = single_gpu_test(model, data_loader)

eval_config = cfg.evaluation
eval_config.pop('interval')
eval_res = dataset.evaluate(outputs, **eval_config)
for name, val in eval_res.items():
    print(f'{name}: {val:.04f}')